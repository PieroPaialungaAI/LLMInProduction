{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f92f649a",
   "metadata": {},
   "source": [
    "# From Wow-Effect to Production: Building Reliable LLM Systems\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The most famous applications of LLMs are the ones that I like to call the \"wow effect LLMs.\" There are plenty of viral LinkedIn posts about them, and they all sound like this:\n",
    "\n",
    "\"I built [x] that does [y] in [z] minutes using AI.\" \n",
    "\n",
    "Where: \n",
    "\n",
    "- **[x]** is usually something like a web app/platform\n",
    "- **[y]** is a somewhat impressive main feature of [x]\n",
    "- **[z]** is usually an integer number between 5 and 10\n",
    "- **\"AI\"** is really, most of the time, a LLM wrapper (Cursor, Codex, or similar)\n",
    "\n",
    "If you notice carefully, the focus of the sentence is not really the quality of the analysis but the amount of time you save. This is to say that, when dealing with a task, people are not excited about the LLM output quality in tackling the problem, but they are thrilled that the LLM is spitting out something quick that might sound like a solution to their problem.\n",
    "\n",
    "This is why I refer to them as **wow-effect LLMs**. As impressive as they sound and look, these wow-effect LLMs display multiple issues that prevent them from being actually implemented in a production environment. Some of them:\n",
    "\n",
    "- **The prompt is usually not optimized**: you don't have time to test all the different versions of the prompts, evaluate them, and provide examples in 5-10 minutes.\n",
    "\n",
    "- **They are not meant to be sustainable**: in that short of time, you can develop a nice-looking plug-and-play wrapper. By default, you are throwing all the costs, latency, maintainability, and privacy considerations out of the window. \n",
    "\n",
    "- **They usually lack context**: LLMs are powerful when they are plugged into a big infrastructure, they have decisional power over the tools that they use, and they have contextual data to augment their answers. No chance of implementing that in 10 minutes. \n",
    "\n",
    "Now, don't get me wrong: LLMs are designed to be intuitive and easy to use. This means that evolving LLMs from the wow effect to production level is not rocket science. However, it requires a specific methodology that needs to be implemented. \n",
    "\n",
    "**The goal of this blog post is to provide this methodology.**\n",
    "\n",
    "The points we will cover to move from wow-effect LLMs to production-level LLMs are the following:\n",
    "\n",
    "1. **LLM System Requirements** - When this beast goes into production, we need to know how to maintain it. This is done in stage zero, through adequate system requirements analysis.  \n",
    "\n",
    "2. **Prompt Engineering** - We are going to optimize the prompt structure and provide some best-practice prompt strategies.\n",
    "\n",
    "3. **Force structure with schemas and structured output** - We are going to move from free text to structured objects, so the format of your response is fixed and reliable.\n",
    "\n",
    "4. **Use tools so the LLM does not work in isolation** - We are going to let the model connect to data and call functions. This provides richer answers and reduces hallucinations.\n",
    "\n",
    "5. **Add guardrails and validation around the model** - Check inputs and outputs, enforce business rules, and define what happens when the model fails or goes out of bounds.\n",
    "\n",
    "6. **Combine everything into a simple, testable pipeline** - Orchestrate prompts, tools, structured outputs, and guardrails into a single flow that you can log, monitor, and improve over time.\n",
    "\n",
    "We are going to use a very simple case: **we are going to make an LLM grade data science tests**. This is just a concrete case to avoid a totally abstract and confusing article. The procedure is general enough to be adapted to other LLM applications, typically with very minor adjustments.\n",
    "\n",
    "Looks like we've got a lot of ground to cover. Let's get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734522ef",
   "metadata": {},
   "source": [
    "## Tough Choices: Cost, Latency, Privacy\n",
    "\n",
    "Before writing any code, there are a few important questions to ask:\n",
    "\n",
    "**How complex is your task?**  \n",
    "Do you really need the latest and most expensive model, or can you use a smaller one or an older family?\n",
    "\n",
    "**How often do you run this, and at what latency?**  \n",
    "Is this a web app that must respond on demand, or a batch job that runs once and stores results? Do users expect an immediate answer, or is \"we'll email you later\" acceptable?\n",
    "\n",
    "**What is your budget?**  \n",
    "You should have a rough idea of what is \"ok to spend\". Is it 1k, 10k, 100k? And compared to that, would it make sense to train and host your own model, or is that clearly overkill?\n",
    "\n",
    "**What are your privacy constraints?**  \n",
    "Is it ok to send this data through an external API? Is the LLM seeing sensitive data? Has this been approved by whoever owns legal and compliance?\n",
    "\n",
    "For simple tasks, where you have a low budget and need low latency, the smaller models (for example the 4.x mini family or 5 nano) are usually your best bet. They are optimized for speed and price, and for many basic use cases like classification, tagging, light transformations, or simple assistants, you will barely notice the quality difference while paying a fraction of the cost.\n",
    "\n",
    "For more complex tasks, such as complex code generation, long-context analysis, or high-stakes evaluations, it can be worth using a stronger model in the 5.x family, even at a higher per-token cost. In those cases, you are explicitly trading money and latency for better decision quality.\n",
    "\n",
    "If you are running large offline workloads, for example re-scoring or re-evaluating thousands of items overnight, batch endpoints can significantly reduce costs compared to real-time calls. This often changes which model fits your budget, because you can afford a \"bigger\" model when latency is not a constraint.\n",
    "\n",
    "From a privacy standpoint, it is also good practice to only send non-sensitive or \"sensitive-cleared\" data to your provider, meaning data that has been cleaned to remove anything confidential or personal. If you need even more control, you can consider running local LLMs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c348ce33",
   "metadata": {},
   "source": [
    "## A LLM Teacher: The Grading Task\n",
    "\n",
    "For this article, we're building an **automated grading system for Data Science exams**. Students take a test that requires them to analyze actual datasets and answer questions based on their findings. The LLM's job is to grade these submissions by:\n",
    "\n",
    "1. Understanding what each question asks\n",
    "2. Accessing the correct answers and grading criteria\n",
    "3. Verifying student calculations against the actual data\n",
    "4. Providing detailed feedback on what went wrong\n",
    "\n",
    "This is a perfect example of why LLMs need tools and context. Without access to the datasets and grading rubrics, the LLM cannot grade accurately. It needs to retrieve the actual data to verify whether a student's answer is correct.\n",
    "\n",
    "### The Test Structure\n",
    "\n",
    "Our exam is stored in `test.json` and contains 10 questions across three sections. Students must analyze three different datasets: e-commerce sales, customer demographics, and A/B test results. Let's look at a few example questions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a76d2f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ EXAMPLE QUESTIONS FROM THE EXAM\n",
      "\n",
      "======================================================================\n",
      "\n",
      "ðŸ”¹ Section A: E-COMMERCE ANALYSIS\n",
      "   Dataset: ecommerce_sales.csv\n",
      "\n",
      "   Question 1 (10 points):\n",
      "   What is the total revenue generated from the \"Electronics\" category in Q4 2024? Show your calculation.\n",
      "\n",
      "\n",
      "ðŸ”¹ Section B: CUSTOMER SEGMENTATION\n",
      "   Dataset: customer_data.csv\n",
      "\n",
      "   Question 5 (10 points):\n",
      "   How many customers fall into each age group? Young (18-30), Middle (31-50), Senior (51+). Provide exact counts for each segment.\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the test file\n",
    "with open('data/test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# Display a few example questions\n",
    "print(\"ðŸ“ EXAMPLE QUESTIONS FROM THE EXAM\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for section in test_data['sections'][:2]:  # Show first 2 sections\n",
    "    print(f\"\\nðŸ”¹ Section {section['section']}: {section['title']}\")\n",
    "    print(f\"   Dataset: {section['dataset']}\")\n",
    "    print()\n",
    "    \n",
    "    # Show first question from each section\n",
    "    question = section['questions'][0]\n",
    "    print(f\"   Question {question['question_number']} ({question['points']} points):\")\n",
    "    print(f\"   {question['question']}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f0c819",
   "metadata": {},
   "source": [
    "### The LLM's Tools: What It Needs to Access\n",
    "\n",
    "Here's the critical insight: **the LLM cannot grade these questions from memory alone**. It needs access to:\n",
    "\n",
    "1. **The Datasets** (`data/datasets/`) - Three CSV files containing the actual data\n",
    "2. **Grading Rubric** (`data/class_resources/grading_rubric.json`) - Defines how to grade\n",
    "3. **Ground Truth Answers** (`data/class_resources/ground_truth_answers.json`) - Contains correct answers\n",
    "\n",
    "Without these tools, the LLM would just be guessing. Let's preview the datasets:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb998ef",
   "metadata": {},
   "source": [
    "## Prompt Engineering: From Vague to Precise\n",
    "\n",
    "A \"wow-effect\" prompt might look like this:\n",
    "\n",
    "> \"Grade this student answer: $6,500\"\n",
    "\n",
    "This is terrible for production. The LLM doesn't know what question, what rubric, what data to check, or how to provide feedback.\n",
    "\n",
    "A **production-ready prompt** implements these key components:\n",
    "\n",
    "1. **Clear Role Definition** - WHO the LLM is and WHAT expertise it has\n",
    "2. **System vs User Messages** - System = standing instructions, User = specific task\n",
    "3. **Explicit Rules with Chain-of-Thought** - Step-by-step reasoning triggers\n",
    "4. **Few-Shot Examples** - Show the LLM correct grading examples\n",
    "\n",
    "We've created all of this in `prompt.py`. Let's examine each component:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa99ab50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM PROMPT (Role + Rules)\n",
      "======================================================================\n",
      "You are an expert Data Science instructor and grader with 10+ years of experience evaluating student work.\n",
      "\n",
      "Your role is to grade student exam submissions fairly, accurately, and with detailed feedback. You must:\n",
      "\n",
      "1. Be objective and consistent in your grading\n",
      "2. Always verify calculations against the actual datasets\n",
      "3. Award partial credit for correct methodology even if the final answer is wrong\n",
      "4. Provide specific, actionable feedback that helps students learn\n",
      "5. Reference exact data points when explaining errors\n",
      "\n",
      "CRITICAL RULES:\n",
      "\n",
      "Rule 1 - ALWAYS Access the Data\n",
      "You MUST access the actual CSV datasets to verify student calculations. Never guess or estimate. If a student claims \"Revenue was $6,500\", you must query the dataset to confirm the actual value.\n",
      "\n",
      "Rule 2 - Follow the Grading Rubric\n",
      "Access the grading_rubric.json file for each question. It specifies:\n",
      "- Point allocation (correct answer, showing work, interpretation)\n",
      "- Partial credit criteria\n",
      "- Common errors to check for\n",
      "\n",
      "Rule 3 - Compare Against Ground Truth\n",
      "Access ground_truth_answers.json to get the correct answer and methodology. Compare the student's approach and results against this reference.\n",
      "\n",
      "Rule 4 - Use Chain-of-Thought Reasoning\n",
      "For each question, think step-by-step:\n",
      "a) What is the question asking?\n",
      "b) What is the correct answer (from ground truth)?\n",
      "c) What did the student answer?\n",
      "d) Is the student's answer correct?\n",
      "e) If wrong, what specifically is incorrect?\n",
      "f) Did they show their work?\n",
      "g) What points should they receive?\n",
      "\n",
      "Rule 5 - Provide Detailed Feedback\n",
      "When an answer is wrong, explain:\n",
      "- What the correct answer is\n",
      "- Why the student's answer is incorrect\n",
      "- What specific data they should have used\n",
      "- Which calculation steps they missed or did wrong\n",
      "\n",
      "Rule 6 - Be Encouraging but Honest\n",
      "Acknowledge correct work, but don't inflate grades. Students learn from honest feedback.\n",
      "\n",
      "USER PROMPT TEMPLATE (Specific Task)\n",
      "======================================================================\n",
      "Grade the following student submission for Question {question_number}.\n",
      "\n",
      "QUESTION:\n",
      "{question_text}\n",
      "\n",
      "STUDENT'S ANSWER:\n",
      "{student_answer}\n",
      "\n",
      "INSTRUCTIONS:\n",
      "1. Access the grading rubric for Question {question_number}\n",
      "2. Access the ground truth answer for Question {question_number}\n",
      "3. Access the relevant dataset: {dataset_file}\n",
      "4. Verify the student's answer against the actual data\n",
      "5. Determine the points earned (out of {max_points})\n",
      "6. Provide detailed feedback\n",
      "\n",
      "Think through this step-by-step following your Chain-of-Thought process.\n"
     ]
    }
   ],
   "source": [
    "# Load and display our prompt structure\n",
    "from prompt import SYSTEM_PROMPT, USER_PROMPT_TEMPLATE, FEW_SHOT_EXAMPLES\n",
    "\n",
    "print(\"SYSTEM PROMPT (Role + Rules)\")\n",
    "print(\"=\"*70)\n",
    "print(SYSTEM_PROMPT)\n",
    "\n",
    "print(\"\\nUSER PROMPT TEMPLATE (Specific Task)\")\n",
    "print(\"=\"*70)\n",
    "print(USER_PROMPT_TEMPLATE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
