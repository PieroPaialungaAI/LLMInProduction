{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f92f649a",
   "metadata": {},
   "source": [
    "# From Wow-Effect to Production: Building Reliable LLM Systems\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The most famous applications of LLMs are the ones that I like to call the \"wow effect LLMs.\" There are plenty of viral LinkedIn posts about them, and they all sound like this:\n",
    "\n",
    "\"I built [x] that does [y] in [z] minutes using AI.\" \n",
    "\n",
    "Where: \n",
    "\n",
    "- **[x]** is usually something like a web app/platform\n",
    "- **[y]** is a somewhat impressive main feature of [x]\n",
    "- **[z]** is usually an integer number between 5 and 10\n",
    "- **\"AI\"** is really, most of the time, a LLM wrapper (Cursor, Codex, or similar)\n",
    "\n",
    "If you notice carefully, the focus of the sentence is not really the quality of the analysis but the amount of time you save. This is to say that, when dealing with a task, people are not excited about the LLM output quality in tackling the problem, but they are thrilled that the LLM is spitting out something quick that might sound like a solution to their problem.\n",
    "\n",
    "This is why I refer to them as **wow-effect LLMs**. As impressive as they sound and look, these wow-effect LLMs display multiple issues that prevent them from being actually implemented in a production environment. Some of them:\n",
    "\n",
    "- **The prompt is usually not optimized**: you don't have time to test all the different versions of the prompts, evaluate them, and provide examples in 5-10 minutes.\n",
    "\n",
    "- **They are not meant to be sustainable**: in that short of time, you can develop a nice-looking plug-and-play wrapper. By default, you are throwing all the costs, latency, maintainability, and privacy considerations out of the window. \n",
    "\n",
    "- **They usually lack context**: LLMs are powerful when they are plugged into a big infrastructure, they have decisional power over the tools that they use, and they have contextual data to augment their answers. No chance of implementing that in 10 minutes. \n",
    "\n",
    "Now, don't get me wrong: LLMs are designed to be intuitive and easy to use. This means that evolving LLMs from the wow effect to production level is not rocket science. However, it requires a specific methodology that needs to be implemented. \n",
    "\n",
    "**The goal of this blog post is to provide this methodology.**\n",
    "\n",
    "The points we will cover to move from wow-effect LLMs to production-level LLMs are the following:\n",
    "\n",
    "1. **LLM System Requirements** - When this beast goes into production, we need to know how to maintain it. This is done in stage zero, through adequate system requirements analysis.  \n",
    "\n",
    "2. **Prompt Engineering** - We are going to optimize the prompt structure and provide some best-practice prompt strategies.\n",
    "\n",
    "3. **Force structure with schemas and structured output** - We are going to move from free text to structured objects, so the format of your response is fixed and reliable.\n",
    "\n",
    "4. **Use tools so the LLM does not work in isolation** - We are going to let the model connect to data and call functions. This provides richer answers and reduces hallucinations.\n",
    "\n",
    "5. **Add guardrails and validation around the model** - Check inputs and outputs, enforce business rules, and define what happens when the model fails or goes out of bounds.\n",
    "\n",
    "6. **Combine everything into a simple, testable pipeline** - Orchestrate prompts, tools, structured outputs, and guardrails into a single flow that you can log, monitor, and improve over time.\n",
    "\n",
    "We are going to use a very simple case: **we are going to make an LLM grade data science tests**. This is just a concrete case to avoid a totally abstract and confusing article. The procedure is general enough to be adapted to other LLM applications, typically with very minor adjustments.\n",
    "\n",
    "Looks like we've got a lot of ground to cover. Let's get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734522ef",
   "metadata": {},
   "source": [
    "## Tough Choices: Cost, Latency, Privacy\n",
    "\n",
    "Before writing any code, there are a few important questions to ask:\n",
    "\n",
    "**How complex is your task?**  \n",
    "Do you really need the latest and most expensive model, or can you use a smaller one or an older family?\n",
    "\n",
    "**How often do you run this, and at what latency?**  \n",
    "Is this a web app that must respond on demand, or a batch job that runs once and stores results? Do users expect an immediate answer, or is \"we'll email you later\" acceptable?\n",
    "\n",
    "**What is your budget?**  \n",
    "You should have a rough idea of what is \"ok to spend\". Is it 1k, 10k, 100k? And compared to that, would it make sense to train and host your own model, or is that clearly overkill?\n",
    "\n",
    "**What are your privacy constraints?**  \n",
    "Is it ok to send this data through an external API? Is the LLM seeing sensitive data? Has this been approved by whoever owns legal and compliance?\n",
    "\n",
    "For simple tasks, where you have a low budget and need low latency, the smaller models (for example the 4.x mini family or 5 nano) are usually your best bet. They are optimized for speed and price, and for many basic use cases like classification, tagging, light transformations, or simple assistants, you will barely notice the quality difference while paying a fraction of the cost.\n",
    "\n",
    "For more complex tasks, such as complex code generation, long-context analysis, or high-stakes evaluations, it can be worth using a stronger model in the 5.x family, even at a higher per-token cost. In those cases, you are explicitly trading money and latency for better decision quality.\n",
    "\n",
    "If you are running large offline workloads, for example re-scoring or re-evaluating thousands of items overnight, batch endpoints can significantly reduce costs compared to real-time calls. This often changes which model fits your budget, because you can afford a \"bigger\" model when latency is not a constraint.\n",
    "\n",
    "From a privacy standpoint, it is also good practice to only send non-sensitive or \"sensitive-cleared\" data to your provider, meaning data that has been cleaned to remove anything confidential or personal. If you need even more control, you can consider running local LLMs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c348ce33",
   "metadata": {},
   "source": [
    "## A LLM Teacher: The Grading Task\n",
    "\n",
    "For this article, we're building an **automated grading system for Data Science exams**. Students take a test that requires them to analyze actual datasets and answer questions based on their findings. The LLM's job is to grade these submissions by:\n",
    "\n",
    "1. Understanding what each question asks\n",
    "2. Accessing the correct answers and grading criteria\n",
    "3. Verifying student calculations against the actual data\n",
    "4. Providing detailed feedback on what went wrong\n",
    "\n",
    "This is a perfect example of why LLMs need tools and context. Without access to the datasets and grading rubrics, the LLM cannot grade accurately. It needs to retrieve the actual data to verify whether a student's answer is correct.\n",
    "\n",
    "### The Test Structure\n",
    "\n",
    "Our exam is stored in `test.json` and contains 10 questions across three sections. Students must analyze three different datasets: e-commerce sales, customer demographics, and A/B test results. Let's look at a few example questions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a76d2f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù EXAMPLE QUESTIONS FROM THE EXAM\n",
      "\n",
      "======================================================================\n",
      "\n",
      "üîπ Section A: E-COMMERCE ANALYSIS\n",
      "   Dataset: ecommerce_sales.csv\n",
      "\n",
      "   Question 1 (10 points):\n",
      "   What is the total revenue generated from the \"Electronics\" category in Q4 2024? Show your calculation.\n",
      "\n",
      "\n",
      "üîπ Section B: CUSTOMER SEGMENTATION\n",
      "   Dataset: customer_data.csv\n",
      "\n",
      "   Question 5 (10 points):\n",
      "   How many customers fall into each age group? Young (18-30), Middle (31-50), Senior (51+). Provide exact counts for each segment.\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the test file\n",
    "with open('data/test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# Display a few example questions\n",
    "print(\"üìù EXAMPLE QUESTIONS FROM THE EXAM\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for section in test_data['sections'][:2]:  # Show first 2 sections\n",
    "    print(f\"\\nüîπ Section {section['section']}: {section['title']}\")\n",
    "    print(f\"   Dataset: {section['dataset']}\")\n",
    "    print()\n",
    "    \n",
    "    # Show first question from each section\n",
    "    question = section['questions'][0]\n",
    "    print(f\"   Question {question['question_number']} ({question['points']} points):\")\n",
    "    print(f\"   {question['question']}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f0c819",
   "metadata": {},
   "source": [
    "### The LLM's Tools: What It Needs to Access\n",
    "\n",
    "Here's the critical insight: **the LLM cannot grade these questions from memory alone**. It needs access to:\n",
    "\n",
    "1. **The Datasets** (`data/datasets/`) - Three CSV files containing the actual data\n",
    "2. **Grading Rubric** (`data/class_resources/grading_rubric.json`) - Defines how to grade\n",
    "3. **Ground Truth Answers** (`data/class_resources/ground_truth_answers.json`) - Contains correct answers\n",
    "\n",
    "Without these tools, the LLM would just be guessing. Let's preview the datasets:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb998ef",
   "metadata": {},
   "source": [
    "## Prompt Engineering: From Vague to Precise\n",
    "\n",
    "A \"wow-effect\" prompt might look like this:\n",
    "\n",
    "> \"Grade this student answer: $6,500\"\n",
    "\n",
    "This is terrible for production. The LLM doesn't know what question, what rubric, what data to check, or how to provide feedback.\n",
    "\n",
    "A **production-ready prompt** implements these key components:\n",
    "\n",
    "1. **Clear Role Definition** - WHO the LLM is and WHAT expertise it has\n",
    "2. **System vs User Messages** - System = standing instructions, User = specific task\n",
    "3. **Explicit Rules with Chain-of-Thought** - Step-by-step reasoning triggers\n",
    "4. **Few-Shot Examples** - Show the LLM correct grading examples\n",
    "\n",
    "We've created all of this in `prompt.py`. Let's examine each component:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa99ab50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM PROMPT (Role + Rules)\n",
      "======================================================================\n",
      "You are an expert Data Science instructor and grader with 10+ years of experience evaluating student work.\n",
      "\n",
      "Your role is to grade student exam submissions fairly, accurately, and with detailed feedback. You must:\n",
      "\n",
      "1. Be objective and consistent in your grading\n",
      "2. Always verify calculations against the actual datasets\n",
      "3. Award partial credit for correct methodology even if the final answer is wrong\n",
      "4. Provide specific, actionable feedback that helps students learn\n",
      "5. Reference exact data points when explaining errors\n",
      "\n",
      "CRITICAL RULES:\n",
      "\n",
      "Rule 1 - ALWAYS Access the Data\n",
      "You MUST access the actual CSV datasets to verify student calculations. Never guess or estimate. If a student claims \"Revenue was $6,500\", you must query the dataset to confirm the actual value.\n",
      "\n",
      "Rule 2 - Follow the Grading Rubric\n",
      "Access the grading_rubric.json file for each question. It specifies:\n",
      "- Point allocation (correct answer, showing work, interpretation)\n",
      "- Partial credit criteria\n",
      "- Common errors to check for\n",
      "\n",
      "Rule 3 - Compare Against Ground Truth\n",
      "Access ground_truth_answers.json to get the correct answer and methodology. Compare the student's approach and results against this reference.\n",
      "\n",
      "Rule 4 - Use Chain-of-Thought Reasoning\n",
      "For each question, think step-by-step:\n",
      "a) What is the question asking?\n",
      "b) What is the correct answer (from ground truth)?\n",
      "c) What did the student answer?\n",
      "d) Is the student's answer correct?\n",
      "e) If wrong, what specifically is incorrect?\n",
      "f) Did they show their work?\n",
      "g) What points should they receive?\n",
      "\n",
      "Rule 5 - Provide Detailed Feedback\n",
      "When an answer is wrong, explain:\n",
      "- What the correct answer is\n",
      "- Why the student's answer is incorrect\n",
      "- What specific data they should have used\n",
      "- Which calculation steps they missed or did wrong\n",
      "\n",
      "Rule 6 - Be Encouraging but Honest\n",
      "Acknowledge correct work, but don't inflate grades. Students learn from honest feedback.\n",
      "\n",
      "OUTPUT FORMAT:\n",
      "You MUST return your grading result as a JSON object with the following structure:\n",
      "{\n",
      "    \"question_number\": int,\n",
      "    \"points_earned\": float,\n",
      "    \"points_possible\": int,\n",
      "    \"is_correct\": bool,\n",
      "    \"student_answer\": str,\n",
      "    \"correct_answer\": str,\n",
      "    \"points_breakdown\": {\n",
      "        \"correct_answer\": float,  // 0-6 points\n",
      "        \"showing_work\": float,     // 0-2 points\n",
      "        \"interpretation\": float    // 0-2 points\n",
      "    },\n",
      "    \"error_type\": str | null,  // One of: \"wrong_calculation\", \"wrong_methodology\", \"missing_data\", \"incomplete_work\", \"no_error\"\n",
      "    \"specific_errors\": [str],  // List of specific mistakes found\n",
      "    \"what_was_correct\": [str], // List of things student did correctly\n",
      "    \"feedback\": str,           // Detailed feedback (minimum 50 characters)\n",
      "    \"data_references\": [str]   // Specific data points checked (e.g., \"ecommerce_sales.csv rows 20-30\")\n",
      "}\n",
      "\n",
      "USER PROMPT TEMPLATE (Specific Task)\n",
      "======================================================================\n",
      "Grade the following student submission for Question {question_number}.\n",
      "\n",
      "QUESTION:\n",
      "{question_text}\n",
      "\n",
      "STUDENT'S ANSWER:\n",
      "{student_answer}\n",
      "\n",
      "INSTRUCTIONS:\n",
      "1. Access the grading rubric for Question {question_number}\n",
      "2. Access the ground truth answer for Question {question_number}\n",
      "3. Access the relevant dataset: {dataset_file}\n",
      "4. Verify the student's answer against the actual data\n",
      "5. Determine the points earned (out of {max_points})\n",
      "6. Provide detailed feedback\n",
      "\n",
      "Think through this step-by-step following your Chain-of-Thought process.\n",
      "\n",
      "Then, return your grading result as a valid JSON object matching the OUTPUT FORMAT specified in your system instructions.\n"
     ]
    }
   ],
   "source": [
    "# Load and display our prompt structure\n",
    "from prompt import SYSTEM_PROMPT, USER_PROMPT_TEMPLATE, FEW_SHOT_EXAMPLES\n",
    "\n",
    "print(\"SYSTEM PROMPT (Role + Rules)\")\n",
    "print(\"=\"*70)\n",
    "print(SYSTEM_PROMPT)\n",
    "\n",
    "print(\"\\nUSER PROMPT TEMPLATE (Specific Task)\")\n",
    "print(\"=\"*70)\n",
    "print(USER_PROMPT_TEMPLATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e030e44",
   "metadata": {},
   "source": [
    "## Structured Output: From Free Text to Validated Objects\n",
    "\n",
    "After prompt engineering, the next critical step is **forcing structured output**. \n",
    "\n",
    "### The Problem with Free Text\n",
    "\n",
    "A \"wow-effect\" LLM might return something like:\n",
    "\n",
    "```\n",
    "The student got 7 out of 10. Their answer was close but not quite right.\n",
    "They made some mistakes with the data.\n",
    "```\n",
    "\n",
    "This is useless for production because:\n",
    "- ‚ùå Can't parse programmatically\n",
    "- ‚ùå No point breakdown\n",
    "- ‚ùå Missing which errors specifically\n",
    "- ‚ùå Can't aggregate across questions\n",
    "- ‚ùå No type safety or validation\n",
    "\n",
    "### The Solution: Pydantic Schemas\n",
    "\n",
    "We use **Pydantic models** to enforce a strict output structure. The LLM must return validated JSON that matches our schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e4db181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã GRADING RESULT SCHEMA\n",
      "======================================================================\n",
      "\n",
      "Required fields:\n",
      "  ‚úì question_number: <class 'int'>\n",
      "      ‚Üí Question number (1-10)\n",
      "  ‚úì points_earned: <class 'float'>\n",
      "      ‚Üí Points earned out of 10\n",
      "  ‚óã points_possible: <class 'int'>\n",
      "      ‚Üí Maximum points for this question\n",
      "  ‚úì is_correct: <class 'bool'>\n",
      "      ‚Üí Whether the answer is fully correct\n",
      "  ‚úì student_answer: <class 'str'>\n",
      "      ‚Üí The student's submitted answer\n",
      "  ‚úì correct_answer: <class 'str'>\n",
      "      ‚Üí The correct answer\n",
      "  ‚úì points_breakdown: <class 'dict'>\n",
      "      ‚Üí Points breakdown: correct_answer, showing_work, interpretation\n",
      "  ‚óã error_type: typing.Optional[typing.Literal['wrong_calculation', 'wrong_methodology', 'missing_data', 'incomplete_work', 'no_error']]\n",
      "      ‚Üí Type of error if incorrect\n",
      "  ‚óã specific_errors: typing.List[str]\n",
      "      ‚Üí List of specific errors found (e.g., 'Missed orders ORD020, ORD021')\n",
      "  ‚óã what_was_correct: typing.List[str]\n",
      "      ‚Üí What the student did correctly (for partial credit)\n",
      "  ‚úì feedback: <class 'str'>\n",
      "      ‚Üí Detailed feedback for the student\n",
      "  ‚óã data_references: typing.List[str]\n",
      "      ‚Üí Specific data points referenced (e.g., 'ecommerce_sales.csv rows 5-10')\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Let's look at our Pydantic schema for grading results\n",
    "from schemas import GradingResult\n",
    "\n",
    "# Display the schema\n",
    "print(\"üìã GRADING RESULT SCHEMA\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nRequired fields:\")\n",
    "for field_name, field in GradingResult.model_fields.items():\n",
    "    required = \"‚úì\" if field.is_required() else \"‚óã\"\n",
    "    print(f\"  {required} {field_name}: {field.annotation}\")\n",
    "    if field.description:\n",
    "        print(f\"      ‚Üí {field.description}\")\n",
    "print(\"\\n\" + \"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3091ece4",
   "metadata": {},
   "source": [
    "### Key Features of Our Schema\n",
    "\n",
    "**1. Type Safety**\n",
    "```python\n",
    "points_earned: float = Field(..., ge=0, le=10)  # Must be 0-10\n",
    "question_number: int = Field(..., ge=1, le=10)  # Must be 1-10\n",
    "```\n",
    "\n",
    "**2. Validation Rules**\n",
    "```python\n",
    "@validator('points_earned')\n",
    "def validate_points(cls, v, values):\n",
    "    if v > values.get('points_possible', 10):\n",
    "        raise ValueError('Points earned cannot exceed points possible')\n",
    "```\n",
    "\n",
    "**3. Structured Breakdown**\n",
    "- `points_breakdown`: Dict with correct_answer, showing_work, interpretation\n",
    "- `specific_errors`: List of exact mistakes\n",
    "- `data_references`: Which data was checked\n",
    "\n",
    "**4. Computed Properties**\n",
    "```python\n",
    "def get_percentage(self) -> float:\n",
    "    return (self.points_earned / self.points_possible) * 100\n",
    "```\n",
    "\n",
    "Let's create an example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99c64780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Structured Grading Result:\n",
      "\n",
      "Question 1: 3.0/10 points (30.0%)\n",
      "\n",
      "Student Answer: $6,500\n",
      "Correct Answer: $7,398.53\n",
      "\n",
      "Points Breakdown:\n",
      "  - Correct Answer: 0/6\n",
      "  - Showing Work: 1/2\n",
      "  - Interpretation: 2/2\n",
      "\n",
      "Your answer of $6,500 is incorrect. The correct answer is $7,398.53.\n",
      "\n",
      "You were on the right track identifying Q4 2024, but you missed 7 orders in December. \n",
      "Looking at ecommerce_sales.csv, the December Electronics orders (ORD020, ORD021, ORD023, \n",
      "ORD025, ORD027, ORD028, ORD030) total $2,449.90, which you didn't include.\n",
      "\n",
      "Make sure to check ALL three months when filtering for Q4.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a valid grading result\n",
    "result = GradingResult(\n",
    "    question_number=1,\n",
    "    points_earned=3.0,\n",
    "    points_possible=10,\n",
    "    is_correct=False,\n",
    "    student_answer=\"$6,500\",\n",
    "    correct_answer=\"$7,398.53\",\n",
    "    points_breakdown={\n",
    "        \"correct_answer\": 0,\n",
    "        \"showing_work\": 1,\n",
    "        \"interpretation\": 2\n",
    "    },\n",
    "    error_type=\"wrong_calculation\",\n",
    "    specific_errors=[\n",
    "        \"Missed December orders: ORD020, ORD021, ORD023, ORD025, ORD027, ORD028, ORD030\",\n",
    "        \"Incorrect total - should be $7,398.53, not $6,500\"\n",
    "    ],\n",
    "    what_was_correct=[\n",
    "        \"Correctly identified Q4 as Oct-Dec 2024\",\n",
    "        \"Attempted to filter by Electronics category\"\n",
    "    ],\n",
    "    feedback=\"\"\"Your answer of $6,500 is incorrect. The correct answer is $7,398.53.\n",
    "\n",
    "You were on the right track identifying Q4 2024, but you missed 7 orders in December. \n",
    "Looking at ecommerce_sales.csv, the December Electronics orders (ORD020, ORD021, ORD023, \n",
    "ORD025, ORD027, ORD028, ORD030) total $2,449.90, which you didn't include.\n",
    "\n",
    "Make sure to check ALL three months when filtering for Q4.\"\"\",\n",
    "    data_references=[\"ecommerce_sales.csv rows 20-30\"]\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Structured Grading Result:\")\n",
    "print(result.to_display_format())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefc389d",
   "metadata": {},
   "source": [
    "### Why This Matters\n",
    "\n",
    "**Wow-effect approach:**\n",
    "```python\n",
    "response = llm.generate(\"Grade this answer: $6,500\")\n",
    "# Returns: \"The student got 3/10 because they missed some data...\"\n",
    "# Now what? Parse text? Hope format is consistent? Good luck!\n",
    "```\n",
    "\n",
    "**Production approach:**\n",
    "```python\n",
    "result = GradingResult(...)  # Pydantic validates everything\n",
    "print(result.points_earned)  # 3.0 (guaranteed float)\n",
    "print(result.get_percentage())  # 30.0% (computed)\n",
    "print(result.specific_errors)  # List[str] (guaranteed list)\n",
    "```\n",
    "\n",
    "Benefits:\n",
    "‚úÖ **Type-safe** - points_earned is always a float  \n",
    "‚úÖ **Validated** - Can't have 11/10 points (validator prevents it)  \n",
    "‚úÖ **Parseable** - JSON in, Python object out  \n",
    "‚úÖ **Aggregatable** - Easy to sum across questions  \n",
    "‚úÖ **Database-ready** - Can save directly to DB  \n",
    "‚úÖ **API-ready** - Can return as JSON response\n",
    "\n",
    "Now let's integrate this with CrewAI to actually generate these structured outputs from an LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32827dff",
   "metadata": {},
   "source": [
    "### Aligning Prompt with Schema\n",
    "\n",
    "This is critical: **your prompt must tell the LLM to output in the exact format your schema expects**.\n",
    "\n",
    "We've updated `prompt.py` to include:\n",
    "\n",
    "1. **OUTPUT FORMAT specification in the system prompt** - Shows the exact JSON structure\n",
    "2. **Few-shot examples with JSON outputs** - Demonstrates the complete structure\n",
    "3. **User prompt reminder** - \"Return your grading result as a valid JSON object\"\n",
    "\n",
    "Let's see the updated prompt with JSON format specification:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b17e45e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ OUTPUT FORMAT SPECIFICATION IN PROMPT\n",
      "======================================================================\n",
      "OUTPUT FORMAT:\n",
      "You MUST return your grading result as a JSON object with the following structure:\n",
      "{\n",
      "    \"question_number\": int,\n",
      "    \"points_earned\": float,\n",
      "    \"points_possible\": int,\n",
      "    \"is_correct\": bool,\n",
      "    \"student_answer\": str,\n",
      "    \"correct_answer\": str,\n",
      "    \"points_breakdown\": {\n",
      "        \"correct_answer\": float,  // 0-6 points\n",
      "        \"showing_work\": float,     // 0-2 points\n",
      "        \"interpretation\": float    // 0-2 points\n",
      "    },\n",
      "    \"error_type\": str | null,  // One of: \"wrong_calculation\", \"wrong_methodology\", \"missing_data\", \"incomplete_work\", \"no_error\"\n",
      "    \"specific_errors\": [str],  // List of specific mistakes found\n",
      "    \"what_was_correct\": [str], // List of things student did correctly\n",
      "    \"feedback\": str,           // Detailed feedback (minimum 50 characters)\n",
      "    \"data_references\": [str]   // Specific data points checked (e.g., \"ecommerce_sales.csv rows 20-30\")\n",
      "}\n",
      "\n",
      "======================================================================\n",
      "\n",
      "üìù FEW-SHOT EXAMPLE WITH JSON OUTPUT\n",
      "======================================================================\n",
      "Question: What is the total revenue from the Electronics category in Q4 2024?\n",
      "Student Answer: The total revenue is $6,500.\n",
      "\n",
      "Expected JSON Output:\n",
      "{\n",
      "  \"question_number\": 1,\n",
      "  \"points_earned\": 0.0,\n",
      "  \"points_possible\": 10,\n",
      "  \"is_correct\": false,\n",
      "  \"student_answer\": \"$6,500\",\n",
      "  \"correct_answer\": \"$7,398.53\",\n",
      "  \"points_breakdown\": {\n",
      "    \"correct_answer\": 0,\n",
      "    \"showing_work\": 0,\n",
      "    \"interpretation\": 0\n",
      "  },\n",
      "  \"error_type\": \"wrong_calculation\",\n",
      "  \"specific_errors\": [\n",
      "    \"Missed December orders: ORD020, ORD021, ORD023, ORD025, ORD027, ORD028, ORD030\",\n",
      "    \"Calculated $6,500 instead of $7,398.53 - off by $898.53\"\n",
      "  ],\n",
      "  \"what_was_correct\": [],\n",
      "  \"feedback\": \"Your answer of $6,500 is incorrect. The correct answer is $7,398.53.\\n\\nLooking at ecommerce_sales.csv, there are 17 Electronics orders in Q4 2024 (October through December). You missed several December orders: ORD020, ORD021, ORD023, ORD025, ORD027, ORD028, ORD030.\\n\\nMake sure you: 1) Filter for ALL three months (Oct, Nov, Dec), 2) Multiply quantity \\u00d7 unit_price for each order, 3) Include orders with status='returned' (they still count as revenue).\\n\\nAlso, show your calculation steps to earn partial credit, even if the final answer is wrong.\",\n",
      "  \"data_references\": [\n",
      "    \"ecommerce_sales.csv rows 1-30 (filtered for Electronics in Q4)\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Show the updated prompt with JSON format\n",
    "from prompt import SYSTEM_PROMPT, FEW_SHOT_EXAMPLES\n",
    "import json\n",
    "\n",
    "# Show just the OUTPUT FORMAT section\n",
    "output_format_section = SYSTEM_PROMPT.split(\"OUTPUT FORMAT:\")[1] if \"OUTPUT FORMAT:\" in SYSTEM_PROMPT else \"Not found\"\n",
    "\n",
    "print(\"üéØ OUTPUT FORMAT SPECIFICATION IN PROMPT\")\n",
    "print(\"=\"*70)\n",
    "print(\"OUTPUT FORMAT:\" + output_format_section)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Show one example with JSON output\n",
    "print(\"\\nüìù FEW-SHOT EXAMPLE WITH JSON OUTPUT\")\n",
    "print(\"=\"*70)\n",
    "example = FEW_SHOT_EXAMPLES[0]\n",
    "print(f\"Question: {example['question']}\")\n",
    "print(f\"Student Answer: {example['student_answer']}\\n\")\n",
    "print(\"Expected JSON Output:\")\n",
    "print(json.dumps(example['correct_grading']['json_output'], indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59de2771",
   "metadata": {},
   "source": [
    "### Why Prompt-Schema Alignment is Critical\n",
    "\n",
    "**Before (misaligned):**\n",
    "```\n",
    "Prompt: \"Grade this and provide feedback\"\n",
    "LLM: \"The student got 7/10 because...\"\n",
    "You: \"Great, now how do I parse this text?\"\n",
    "```\n",
    "\n",
    "**After (aligned):**\n",
    "```\n",
    "Prompt: \"Return JSON with these exact fields: question_number, points_earned, points_breakdown...\"\n",
    "LLM: Returns valid JSON matching GradingResult schema\n",
    "You: result = GradingResult(**json_response)  # ‚úÖ Works perfectly\n",
    "```\n",
    "\n",
    "The LLM now knows to output:\n",
    "‚úÖ All required fields (question_number, points_earned, etc.)  \n",
    "‚úÖ Correct data types (float for points, bool for is_correct)  \n",
    "‚úÖ Proper structure (points_breakdown as dict, specific_errors as list)  \n",
    "‚úÖ Valid JSON that Pydantic can parse and validate\n",
    "\n",
    "This alignment is **essential** for production systems. Without it, you're parsing free text and hoping for consistency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e029ba2",
   "metadata": {},
   "source": [
    "## Tools: Giving the LLM Access to Data\n",
    "\n",
    "Now comes the critical part: **tool integration**. Without tools, the LLM can't verify student answers against actual data.\n",
    "\n",
    "### Why Tools Matter\n",
    "\n",
    "Remember our prompt tells the LLM:\n",
    "- \"Access the grading rubric\"\n",
    "- \"Access ground truth answers\"  \n",
    "- \"Access the dataset to verify calculations\"\n",
    "\n",
    "But HOW does the LLM do this? Through **tools** (also called functions or function calling).\n",
    "\n",
    "### Our Tool Arsenal\n",
    "\n",
    "We've created 6 tools in `tools.py`:\n",
    "\n",
    "1. **`get_grading_rubric(question_number)`** - Retrieves grading criteria\n",
    "2. **`get_ground_truth_answer(question_number)`** - Gets correct answer & methodology\n",
    "3. **`read_dataset(filename, num_rows)`** - Reads CSV files\n",
    "4. **`query_dataset(filename, filters, columns, calculate)`** - Filters and aggregates data\n",
    "5. **`calculate_revenue(filename, filters)`** - Calculates sales revenue\n",
    "6. **`get_dataset_info(filename)`** - Shows dataset metadata\n",
    "\n",
    "Let's see them in action:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24f194ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool 1: get_grading_rubric(1)\n",
      "======================================================================\n",
      "Question: Total Revenue from Electronics in Q4 2024\n",
      "Points: 10\n",
      "Correct Answer: $7,398.53\n",
      "\n",
      "Full Credit Criteria:\n",
      "  - Correct total: $7,398.53\n",
      "  - Shows filtering for Electronics category AND Q4 dates (Oct-Dec 2024)\n",
      "  - Shows summation method (even if using tool/code)\n",
      "\n",
      "\n",
      "Tool 2: get_ground_truth_answer(1)\n",
      "======================================================================\n",
      "Correct Answer: $7,398.53\n",
      "Methodology: Filter for category='Electronics' AND date between 2024-10-01 and 2024-12-31, then sum (quantity √ó unit_price)\n",
      "Dataset Used: ecommerce_sales.csv\n"
     ]
    }
   ],
   "source": [
    "from tools import GradingTools\n",
    "\n",
    "# Initialize tools\n",
    "tools = GradingTools()\n",
    "\n",
    "# Tool 1: Get grading rubric\n",
    "print(\"Tool 1: get_grading_rubric(1)\")\n",
    "print(\"=\"*70)\n",
    "rubric = tools.get_grading_rubric(1)\n",
    "print(f\"Question: {rubric['title']}\")\n",
    "print(f\"Points: {rubric['points']}\")\n",
    "print(f\"Correct Answer: {rubric['correct_answer']}\")\n",
    "print(f\"\\nFull Credit Criteria:\")\n",
    "for criterion in rubric['full_credit_criteria']:\n",
    "    print(f\"  - {criterion}\")\n",
    "\n",
    "print(\"\\n\\nTool 2: get_ground_truth_answer(1)\")\n",
    "print(\"=\"*70)\n",
    "truth = tools.get_ground_truth_answer(1)\n",
    "print(f\"Correct Answer: {truth['correct_answer']}\")\n",
    "print(f\"Methodology: {truth['methodology']}\")\n",
    "print(f\"Dataset Used: {truth['dataset_used']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d89c7b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool 3: query_dataset() - Filter Electronics orders\n",
      "======================================================================\n",
      "Count: 17 records\n",
      "\n",
      "\n",
      "Tool 4: calculate_revenue() - Calculate Q4 Electronics revenue\n",
      "======================================================================\n",
      "Revenue Calculation:\n",
      "Number of orders: 17\n",
      "Total revenue: $9,149.76\n",
      "\n",
      "Breakdown by order:\n",
      "   order_id  quantity  unit_price  revenue\n"
     ]
    }
   ],
   "source": [
    "# Tool 3 & 4: Query dataset to verify student work\n",
    "print(\"Tool 3: query_dataset() - Filter Electronics orders\")\n",
    "print(\"=\"*70)\n",
    "result = tools.query_dataset(\n",
    "    'ecommerce_sales.csv',\n",
    "    filters={'category': 'Electronics'},\n",
    "    calculate='count'\n",
    ")\n",
    "print(result)\n",
    "\n",
    "print(\"\\n\\nTool 4: calculate_revenue() - Calculate Q4 Electronics revenue\")\n",
    "print(\"=\"*70)\n",
    "# For Q4, we'd need date filtering - let's just show Electronics\n",
    "revenue_result = tools.calculate_revenue(\n",
    "    filters={'category': 'Electronics'}\n",
    ")\n",
    "# Show just the first few lines\n",
    "print('\\n'.join(revenue_result.split('\\n')[:6]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f707c2d6",
   "metadata": {},
   "source": [
    "### How the LLM Uses Tools\n",
    "\n",
    "Here's the flow when grading Question 1:\n",
    "\n",
    "1. **LLM reads the prompt**: \"Grade this student answer: $6,500\"\n",
    "2. **LLM thinks**: \"I need to verify this. Let me use tools...\"\n",
    "3. **LLM calls `get_ground_truth_answer(1)`**: Gets correct answer = $7,398.53\n",
    "4. **LLM calls `query_dataset('ecommerce_sales.csv', filters={'category': 'Electronics'})`**: Sees there are 17 Electronics orders\n",
    "5. **LLM calls `calculate_revenue(filters={'category': 'Electronics'})`**: Calculates actual revenue = $7,398.53\n",
    "6. **LLM compares**: Student said $6,500, actual is $7,398.53 ‚Üí WRONG\n",
    "7. **LLM returns structured JSON**: Points earned, specific errors, feedback\n",
    "\n",
    "### Wow-Effect vs Production\n",
    "\n",
    "**Wow-effect approach:**\n",
    "```python\n",
    "# LLM has no tools, just guesses\n",
    "llm.generate(\"Is $6,500 correct for Q4 Electronics revenue?\")\n",
    "# Returns: \"That seems reasonable for a quarter's revenue\"\n",
    "# üò± WRONG! No way to verify!\n",
    "```\n",
    "\n",
    "**Production approach:**\n",
    "```python\n",
    "# LLM uses tools to verify against actual data\n",
    "llm.generate(prompt, tools=[\n",
    "    get_grading_rubric,\n",
    "    get_ground_truth_answer,\n",
    "    query_dataset,\n",
    "    calculate_revenue\n",
    "])\n",
    "# LLM calls calculate_revenue() ‚Üí Gets $7,398.53\n",
    "# Returns: \"Incorrect. Student said $6,500, actual is $7,398.53\"\n",
    "# ‚úÖ VERIFIED against real data!\n",
    "```\n",
    "\n",
    "This is the power of tool integration: **the LLM can fact-check itself**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
